{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rJ9SUqJjHEGC",
        "outputId": "a5aba0df-e3da-41ab-b0d5-d032c50b2a86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_NFbnPiNHEGE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EGUHcaVgHEGF"
      },
      "outputs": [],
      "source": [
        "mark_start = 'ssss ' #başlangıç değeri yani modelimize burası başlangıç noktası diyoruz\n",
        "mark_end = ' eeee' #bitiş değeri; modele nerde durması gerektiğini belirtiyoruz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GW0jJNDRHEGG"
      },
      "outputs": [],
      "source": [
        "data_src = [] #veri listelerimiz source kısmına ingilizce yapılar konulacak\n",
        "data_dest = [] #türkçe veriler konulacak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dyys9kNhHEGG"
      },
      "outputs": [],
      "source": [
        "for line in open('tur.txt', encoding='UTF-8'):\n",
        "    en_text, tr_text = line.rstrip().split('\\t')\n",
        "\n",
        "    tr_text = mark_start + tr_text + mark_end #başlangıç ve bitiş işaretlerimiz ekliyoruz \"ssss abc eeee\"\n",
        "\n",
        "    data_src.append(en_text)\n",
        "    data_dest.append(tr_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lV0AHfIxHEGH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d104425-2e16-45a3-e8b6-c3c5ba70249c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I hope so.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data_src[500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uAZm4jMOHEGI",
        "outputId": "f84146c6-c4ef-4267-9080-e54a405d7e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss Öyle olduğunu umuyorum. eeee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data_dest[500] #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3p3ZIrHSHEGI",
        "outputId": "b282d06b-bd8c-4687-b87b-8a6ffad536ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Do you know my name?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data_src[50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yM6JPpHvHEGJ",
        "outputId": "9cdb1ed3-fb97-41ca-8b52-33cdc6f5875d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss Adımı biliyor musun? eeee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data_dest[50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UfMVlA0YHEGL",
        "outputId": "d9bfb77b-6764-426a-a42a-1a3f2c0af5c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "473035"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(data_src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dAb0wuU2HEGL"
      },
      "outputs": [],
      "source": [
        "##TOKENIZER sınıfından kalıtım alarak bazı işlevler ekleyeceğiz##\n",
        "\n",
        "class TokenizerWrap(Tokenizer):\n",
        "    def __init__(self, texts, padding, reverse=False, num_words=None): #init fonksiyonu classımızı her çağırdığımızda çalışacak yapıları belirtir\n",
        "        Tokenizer.__init__(self, num_words=num_words)\n",
        "\n",
        "        self.fit_on_texts(texts)\n",
        "        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys())) #kelimelere index vererek bir dictionary oluşturuyoruz.\n",
        "        self.tokens = self.texts_to_sequences(texts)\n",
        "\n",
        "        if reverse:\n",
        "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
        "            truncating = 'pre'          #pre ve post olarak ikiye ayrılıyor\n",
        "        else:                           #dizileri ne tarafa ekleyeceğimizi nereden keseceğimizi belirtir.\n",
        "            truncating = 'post'\n",
        "\n",
        "\n",
        "        #max uzunluğu belirliyoruz\n",
        "        self.num_tokens = [len(x) for x in self.tokens]\n",
        "        self.max_tokens = np.mean(self.num_tokens) + 2 * np.std(self.num_tokens)\n",
        "        self.max_tokens = int(self.max_tokens)\n",
        "\n",
        "        #padding(ekleme ve silme) işlemi\n",
        "        self.tokens_padded = pad_sequences(self.tokens,\n",
        "                                           maxlen=self.max_tokens,\n",
        "                                           padding=padding,\n",
        "                                           truncating=truncating)\n",
        "\n",
        "    def token_to_word(self, token): #tokenleri metne çevirir.\n",
        "        word = ' ' if token == 0 else self.index_to_word[token]\n",
        "        return word\n",
        "\n",
        "    def tokens_to_string(self, tokens): #token dizisini (sayılar) tekrar metne çevirir.\n",
        "        words = [self.index_to_word[token] for token in tokens if token != 0]\n",
        "        text = ' '.join(words)\n",
        "        return text\n",
        "\n",
        "    def text_to_tokens(self, text, padding, reverse=False): #metni token şeklinde verir\n",
        "        tokens = self.texts_to_sequences([text])\n",
        "        tokens = np.array(tokens)\n",
        "\n",
        "        if reverse:\n",
        "            tokens = np.flip(tokens, axis=1)\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "            truncating = 'post'\n",
        "\n",
        "        tokens = pad_sequences(tokens,\n",
        "                               maxlen=self.max_tokens,\n",
        "                               padding=padding,\n",
        "                               truncating=truncating)\n",
        "\n",
        "        return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kKI3sIv7HEGM"
      },
      "outputs": [],
      "source": [
        "tokenizer_src = TokenizerWrap(texts=data_src,  #oluşturduğumuz \"TokenizerWrap\" sınıfını kullanıyoruz\n",
        "                              padding='pre',   #başına sıfırlar eklenerek doldurulacak.\n",
        "                              reverse=True,    #cümle sırası ters çevrilecek\n",
        "                              num_words=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sHsE4WRLHEGM"
      },
      "outputs": [],
      "source": [
        "tokenizer_dest = TokenizerWrap(texts=data_dest,\n",
        "                              padding='post',  #başına sıfırlar eklenerek doldurulacak.\n",
        "                              reverse=False,   #cümle ters çevirlmeyecek\n",
        "                              num_words=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FZKppxCIHEGN",
        "outputId": "369637a0-15a9-4398-fdf9-f973632d64ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(473035, 11)\n",
            "(473035, 10)\n"
          ]
        }
      ],
      "source": [
        "# token dizilerine erişim #\n",
        "tokens_src = tokenizer_src.tokens_padded\n",
        "tokens_dest = tokenizer_dest.tokens_padded\n",
        "print(tokens_src.shape) #tokenlenmiş source listemizin boyutunu yazdırır\n",
        "print(tokens_dest.shape) #tokenlenmiş destination(türkçe) listemizin boyutu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EBLljp6BHEGN",
        "outputId": "c3e8526f-cbe6-458d-bc5c-423fcc9638fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1, 2233,  108,   48,    2,    0,    0,    0,    0,    0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "tokens_dest[50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nSePtz_xHEGO",
        "outputId": "7c0734b6-8964-42ba-a6cd-86ff33d0b068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss adımı biliyor musun eeee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tokenizer_dest.tokens_to_string(tokens_dest[50000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3qg1NwaAHEGO",
        "outputId": "a8d00a11-3d0e-4733-f4d6-1d46b28f6521",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0, 288,  24,  33,   5,   9], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokens_src[50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N3vFjH4WHEGO",
        "outputId": "186e89a7-9653-4511-8c28-a7edd66c9acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'missing anything see you can'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tokenizer_src.tokens_to_string(tokens_src[200000]) #source lsitemizdeki tokenleştirilmiş cümleleri tekrar metne çevirerek ters çevrilmiş halini verir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "a95DCfcqHEGP",
        "outputId": "d2f1521a-6f49-4532-802f-7ca21148bfe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Do you know my name?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "data_src[50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t4XBQz5AHEGP",
        "outputId": "5f38a517-1cde-4851-9e8e-bd5141faf898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
        "token_start # \"ssss \" başlangıç markımızın indexi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "k4lblao6HEGQ",
        "outputId": "ddd708cd-5e4e-4912-8d1d-d1ef60712207",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
        "token_end # \" eeee\" bitiş markımızın indexi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vXcfbjGLHEGQ"
      },
      "outputs": [],
      "source": [
        "## ENCODER VE DECODER ICIN GIRIS VE CIKIS DEGERLERINI HAZIRLAMA ##\n",
        "\n",
        "encoder_input_data = tokens_src\n",
        "decoder_input_data = tokens_dest[:, :-1] #ürkçe cümlelerin başlangıçtan bir önceki hali\n",
        "decoder_output_data = tokens_dest[:, 1:] #Türkçe cümlelerin bir sonraki hali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bSaEVvGcHEGR",
        "outputId": "2d7ae9a1-0a9f-4419-c012-88ab16f9c78b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0, 288,  24,  33,   5,   9], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "encoder_input_data[50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pL80Di7LHEGS",
        "outputId": "e6793e11-5d7c-4d8c-c1fc-aeae21dced88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1, 2233,  108,   48,    2,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "decoder_input_data[50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MehK_2SMHEGS",
        "outputId": "7ae6f15c-8eeb-4681-c8c7-7a390a098262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2233,  108,   48,    2,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "decoder_output_data[50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UrT1EeDuHEGS",
        "outputId": "59737753-5614-46a6-e9c4-cd5a8475e4da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss adımı biliyor musun eeee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "tokenizer_dest.tokens_to_string(decoder_input_data[50000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "n8X5ET8XHEGT",
        "outputId": "2beec4d0-b397-484c-de28-c35a67706f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'adımı biliyor musun eeee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "tokenizer_dest.tokens_to_string(decoder_output_data[50000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "k0ipOBQdHEGT"
      },
      "outputs": [],
      "source": [
        "num_encoder_words = len(tokenizer_src.word_index)\n",
        "num_decoder_words = len(tokenizer_dest.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xuS7JIFnHEGT",
        "outputId": "99df9bc4-f832-44b5-f56e-907a9abfac44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21315"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "num_encoder_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2wSuLxVTHEGU",
        "outputId": "ded8fbf3-205d-41ca-9dec-925757223243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94058"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "num_decoder_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zEnMd892HEGU"
      },
      "outputs": [],
      "source": [
        "embedding_size = 100 #her kelime 100 boyutlu matrisle ifade edilecek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Wei9BIgVHEGU"
      },
      "outputs": [],
      "source": [
        "word2vec = {}\n",
        "with open('glove.6B.100d.txt', encoding='UTF-8') as f: #glove ile eğitilmil 6 milyar 100 boyutlu ingilizce kelime vektörlerini içerir\n",
        "    for line in f:                                     #her satır: \"word 0.1 0.25 -0.3 ...\" şeklinde\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vec = np.asarray(values[1:], dtype='float32')\n",
        "        word2vec[word] = vec\n",
        "\n",
        "#word2vec sözlüğünü oluşturuyoruz exp: \"cat\": [0.1, 0.25, ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "YVvKU_f1HEGV"
      },
      "outputs": [],
      "source": [
        "# embedding matrixini oluşturma #\n",
        "embedding_matrix = np.random.uniform(-1, 1, (num_encoder_words, embedding_size))\n",
        "for word, i in tokenizer_src.word_index.items(): #ingilizce kelimeler ve onların indeksleri\n",
        "    if i < num_encoder_words:\n",
        "        embedding_vector = word2vec.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector #o kelimeye karşılık gelen indeks pozisyonuna matrix verilir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gTySmKA0HEGV",
        "outputId": "5c17e9fb-8617-4e79-aab3-865e3eb6e66e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21315, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "embedding_matrix.shape #oluşturduğumuz embedding matrisinin boyutu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "E5khhevpHEGV"
      },
      "outputs": [],
      "source": [
        "encoder_input = Input(shape=(None,), name='encoder_input') #encoder yani ingilizce tarafta çalışan kısım\n",
        "                                                           #shape none olması cümle uzunluğu değişken olabilir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ubnVVsDsHEGW"
      },
      "outputs": [],
      "source": [
        "#ingilizce kelimeler embedding vektörlerine çevirme\n",
        "\n",
        "encoder_embedding = Embedding(input_dim=num_encoder_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              weights=[embedding_matrix], #daha önce oluşturduğumuz gloVe matrisini kullanır\n",
        "                              trainable=True,             #eğitim sırasında matrisler güncellenebilir\n",
        "                              name='encoder_embedding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qJ8HKWeEHEGW"
      },
      "outputs": [],
      "source": [
        "#encoder (ingilizce) tarafı için 3 katlı gru\n",
        "state_size = 256\n",
        "encoder_gru1 = GRU(state_size, name='encoder_gru1', return_sequences=True)\n",
        "encoder_gru2 = GRU(state_size, name='encoder_gru2', return_sequences=True)  #true olunca tüm adımların çıktısını döner\n",
        "encoder_gru3 = GRU(state_size, name='encoder_gru3', return_sequences=False) #false olunca sadece son adımın çıktısını döner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "DDDYPsshHEGW"
      },
      "outputs": [],
      "source": [
        "# encoder katmanlarını bağlama #\n",
        "def connect_encoder(encoder_input):\n",
        "    net = encoder_embedding(encoder_input)\n",
        "    net = encoder_gru1(net)\n",
        "    net = encoder_gru2(net)\n",
        "    net = encoder_gru3(net)\n",
        "    return net  # encoder_output\n",
        " #encoder_output : Sabit boyutta bir vektör, Türkçeye çevrilecek İngilizce cümlenin anlam temsili.\n",
        "\n",
        "#tüm encoder katmanlarını sırayla birbirine bağlar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RKeeLmDvHEGX"
      },
      "outputs": [],
      "source": [
        "encoder_output = connect_encoder(encoder_input)  #encoderı çalıştırma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "yxFOcjaEHEGX"
      },
      "outputs": [],
      "source": [
        "decoder_initial_state = Input(shape=(state_size,), name='decoder_initial_state')  #decoder yani türkçe tarafta çalışan kısım"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UVXqynmDHEGX"
      },
      "outputs": [],
      "source": [
        "decoder_input = Input(shape=(None,), name='decoder_input')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7iQBZ0A6HEGY"
      },
      "outputs": [],
      "source": [
        "# decoder (türkçe) kısım için embedding matrisleri oluşturma #\n",
        "decoder_embedding = Embedding(input_dim=num_decoder_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='decoder_embedding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "CLuJGD2THEGY"
      },
      "outputs": [],
      "source": [
        "# 3 katlı gru eğitimi\n",
        "decoder_gru1 = GRU(state_size, name='decoder_gru1', return_sequences=True)\n",
        "decoder_gru2 = GRU(state_size, name='decoder_gru2', return_sequences=True)\n",
        "decoder_gru3 = GRU(state_size, name='decoder_gru3', return_sequences=True) #hepisinin true olmasının sebebi her kelime üretiiminde çıktı üretmesi gerektiğimnden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hI8bfQwsHEGY"
      },
      "outputs": [],
      "source": [
        "# dense yani çıktı katmanı oluşturma #\n",
        "decoder_dense = Dense(num_decoder_words,\n",
        "                      activation='linear',   #türkçedeki toplam kelime sayısı kadar nöron içerir\n",
        "                      name='decoder_output') #Sonrasında softmax uygulanacak, o yüzden burası lineer bırakılır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vPC3c_h2HEGZ"
      },
      "outputs": [],
      "source": [
        "# decoder için gru ları bağlama\n",
        "def connect_decoder(decoder_input, initial_state):\n",
        "    net = decoder_embedding(decoder_input)\n",
        "    net = decoder_gru1(net, initial_state=initial_state)  # İlk katmana encoder’dan gelen state\n",
        "    net = decoder_gru2(net)   # Sonraki katmanlarda initial_state yok\n",
        "    net = decoder_gru3(net)\n",
        "    decoder_output = decoder_dense(net)\n",
        "    return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "sM11PFVKHEGZ"
      },
      "outputs": [],
      "source": [
        "# EĞİTİM MODELİ #\n",
        "# Inputları tanımlama\n",
        "decoder_input = Input(shape=(None,), name='decoder_input')\n",
        "\n",
        "# fonksiyonları kullanarak ağları bağla\n",
        "encoder_output = connect_encoder(encoder_input)\n",
        "decoder_output = connect_decoder(decoder_input, initial_state=encoder_output)\n",
        "\n",
        "# modeli tanımla\n",
        "model_train = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
        "                                                                                     #encoder input ingilizce cümle,\n",
        "                                                                                     #decoder input türkçe cümlenin başlangıç kısmı ssss\n",
        "                                                                                     #decoder output türkçeye çevrilen cümlemiz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "O7Az8SsuHEGZ"
      },
      "outputs": [],
      "source": [
        "model_encoder = Model(inputs=[encoder_input], outputs=[encoder_output]) #Sadece ingilizce tarafı, tahmin aşamasında önce bu çalıştırılır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "psCEomnPHEGa"
      },
      "outputs": [],
      "source": [
        "decoder_output = connect_decoder(decoder_input, initial_state=decoder_initial_state) #decoder input : şu ana kadar üretilen Türkçe kelimeler.\n",
        "\n",
        "model_decoder = Model(inputs=[decoder_input, decoder_initial_state], outputs=[decoder_output]) #decoder_initial_state: encoder çıktısı)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HyNLwSF6HEGb"
      },
      "outputs": [],
      "source": [
        "# LOSS KAYIP FONKSIYONU #\n",
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred) # y_true: gerçek türkçe kelime tokenleri.\n",
        "                                                                                        # y_pred: modelin tahmin ettiği skorlar.\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "    return loss_mean #ortalama kayıp değeri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "EF5zsl-3HEGb"
      },
      "outputs": [],
      "source": [
        "optimizer = RMSprop(learning_rate=1e-3) # öğrenme oranı: 1e-3 : 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "eTN2xGbgHEGb"
      },
      "outputs": [],
      "source": [
        "decoder_target = Input(shape=(None,), dtype='int32', name='decoder_target')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "zfUnWxMGHEGb"
      },
      "outputs": [],
      "source": [
        "model_train.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "SYTQTjoLHEGc"
      },
      "outputs": [],
      "source": [
        "path_checkpoint = 'checkpoint.weights.h5'  #eğitim sırasında modelin ağırlıkları bu dosyaya kaydedilir.\n",
        "checkpoint = ModelCheckpoint(filepath=path_checkpoint, save_weights_only=True) #save_weights_only=True: Sadece weights saklanır model yapısı değil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "QEhtlvitHEGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358781db-a5fc-4137-fff6-72bf7894dbe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.\n",
            "[Errno 2] Unable to synchronously open file (unable to open file: name = 'checkpoint.weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    model_train.load_weights(path_checkpoint)\n",
        "except Exception as error:\n",
        "    print('Checkpoint yüklenirken hata oluştu. Eğitime sıfırdan başlanıyor.')\n",
        "    print(error)\n",
        "#önceden eğitilmiş bir model varsa onu yükler\n",
        "#hata varsa(örneğin dosya yoksa) sıfırdan başlar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "gj-d2sSDHEGc"
      },
      "outputs": [],
      "source": [
        "x_data = {'encoder_input': encoder_input_data, 'decoder_input': decoder_input_data}\n",
        "y_data = {'decoder_output': decoder_output_data}\n",
        "#encoder_input_data: kaynak yani ingilizce cümleler.\n",
        "#decoder_input_data: Türkçe çevirinin giriş kısmı (başlangıç ssss dahil ama son kelime yok)\n",
        "#decoder_output_data: Türkçe çevirinin hedef kısmı (ilk kelime yok ama eeee dahil)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "0Rf1LfQyHEGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee81dcf5-108b-4aaa-e17d-3a1286c51e3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d297e871a50>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "model_train.fit(x=x_data,\n",
        "                y=y_data,\n",
        "                batch_size=256,\n",
        "                epochs=0, #sadece modelin yapısını çalıştırır, bunu 10–20 yaparsak gerçek eğitim başlar.\n",
        "                callbacks=[checkpoint]) #eğitim sırasında her epoch sonunda modeli kaydeder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "3zxbGpfiHEGd"
      },
      "outputs": [],
      "source": [
        "def translate(input_text, true_output_text=None):\n",
        "    # Input text'i tokenize et\n",
        "    input_tokens = tokenizer_src.text_to_tokens(\n",
        "        text=input_text,\n",
        "        reverse=True,\n",
        "        padding='pre'\n",
        "    )\n",
        "\n",
        "    # Eğer input_tokens tek bir cümleyse, batch haline getir\n",
        "    if len(input_tokens.shape) == 1:\n",
        "        input_tokens = np.expand_dims(input_tokens, axis=0)\n",
        "\n",
        "    max_tokens = tokenizer_dest.max_tokens\n",
        "    decoder_input_data = np.zeros((1, max_tokens), dtype=int)  # decoder girişi sıfırlarla başlatıldı.\n",
        "\n",
        "    # Encoder'dan başlangıç durumu alınır\n",
        "    initial_state = model_encoder.predict(input_tokens, verbose=0)\n",
        "    print(f\"Initial state shape: {initial_state.shape}\")\n",
        "    state_size = initial_state.shape[1]  # state_size'i encoder çıkış boyutuna göre ayarlıyoruz\n",
        "    initial_state = initial_state.reshape(1, state_size)  # Hedeflenmiş şekle dönüştürülür.\n",
        "\n",
        "    token_int = token_start  # Başlangıç token'ı\n",
        "    output_text = ''  # Çıktıyı burada biriktireceğiz\n",
        "    count_tokens = 0  # Token sayısı\n",
        "\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        decoder_input_data[0, count_tokens] = token_int  # Şu anki token'ı decoder girişine ekle\n",
        "\n",
        "        # Decoder'a veri gönder\n",
        "        x_data = {\n",
        "            'decoder_input': decoder_input_data[:, :count_tokens + 1],  # Sadece gerekli token'ları ver\n",
        "            'decoder_initial_state': initial_state  # Encoder'dan gelen initial state\n",
        "        }\n",
        "\n",
        "        # Decoder modelinden çıktı alınır\n",
        "        decoder_output = model_decoder.predict(x_data, verbose=0)\n",
        "\n",
        "        # Çıktıdan token alınır\n",
        "        token_onehot = decoder_output[0, count_tokens, :]\n",
        "        token_int = np.argmax(token_onehot)  # En yüksek olasılığa sahip token alınır\n",
        "\n",
        "        # Token'ı kelimeye dönüştür\n",
        "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
        "        output_text += ' ' + sampled_word  # Kelimeyi çıktıya ekle\n",
        "\n",
        "        count_tokens += 1  # Token sayısını artır\n",
        "\n",
        "    # Sonuçları yazdır\n",
        "    print('Input text:')\n",
        "    print(input_text)\n",
        "    print()\n",
        "\n",
        "    print('Translated text:')\n",
        "    print(output_text.strip())\n",
        "    print()\n",
        "\n",
        "    if true_output_text is not None:\n",
        "        print('True output text:')\n",
        "        print(true_output_text)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "_ROWh9i5HEGe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "4533302d-aacf-42fc-827c-49b9ab63efba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state shape: (1, 128)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 1 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 256), found shape=(1, 128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-98b40549ea30>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_src\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m400000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_output_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m400000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-f9b781a163ba>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(input_text, true_output_text)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Decoder modelinden çıktı alınır\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Çıktıdan token alınır\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    246\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 1 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 256), found shape=(1, 128)"
          ]
        }
      ],
      "source": [
        "translate(input_text=data_src[400000], true_output_text=data_dest[400000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qWD5XSTLHEGe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbf9fe08-7e83-416e-d817-523da36b4e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['encoder_input']\n",
            "Received: inputs=Tensor(shape=(1, 11))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node functional_2_1/embedding_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-61-a06638c2a9e3>\", line 1, in <cell line: 0>\n\n  File \"<ipython-input-59-f9b781a163ba>\", line 17, in translate\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 562, in predict\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 259, in one_step_on_data_distributed\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 249, in one_step_on_data\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 104, in predict_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 637, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py\", line 140, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/numpy.py\", line 5346, in take\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/numpy.py\", line 2093, in take\n\nindices[0,8] = 4067 is not in [0, 1000)\n\t [[{{node functional_2_1/embedding_1/GatherV2}}]] [Op:__inference_one_step_on_data_distributed_1435]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-a06638c2a9e3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Which road leads to the airport?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-f9b781a163ba>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(input_text, true_output_text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Encoder'dan başlangıç durumu alınır\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initial state shape: {initial_state.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mstate_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# state_size'i encoder çıkış boyutuna göre ayarlıyoruz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node functional_2_1/embedding_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-61-a06638c2a9e3>\", line 1, in <cell line: 0>\n\n  File \"<ipython-input-59-f9b781a163ba>\", line 17, in translate\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 562, in predict\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 259, in one_step_on_data_distributed\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 249, in one_step_on_data\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 104, in predict_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 637, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py\", line 140, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/numpy.py\", line 5346, in take\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/numpy.py\", line 2093, in take\n\nindices[0,8] = 4067 is not in [0, 1000)\n\t [[{{node functional_2_1/embedding_1/GatherV2}}]] [Op:__inference_one_step_on_data_distributed_1435]"
          ]
        }
      ],
      "source": [
        "translate(input_text='Which road leads to the airport?')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}